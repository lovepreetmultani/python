{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRZAV02laRPmfSKBoM9DL4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lovepreetmultani/python/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfllbKaqbyR6",
        "outputId": "8859b7b8-2665-4bc1-833a-d2fefa7ecf3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming with stopwords**"
      ],
      "metadata": {
        "id": "6ZeBzzJ7kk-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"Mr. Green killed Colonel Mustard in the study with the candlestick. Mr. Green is not a very nice fellow,\n",
        "    Professor Plum has a green plant in his study,\n",
        "    Miss Scarlett watered Professor Plum's green plant while he was away from his office last week.\"\"\""
      ],
      "metadata": {
        "id": "S2Ege5ktcP4d"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_tokenized = nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "lR-_LMlacYjj"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "VK3JhIU4gVWj"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('english')"
      ],
      "metadata": {
        "id": "V3XWuwGthons"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences_tokenized)):\n",
        "  words = nltk.word_tokenize(sentences_tokenized[i])\n",
        "  words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  sentences_tokenized[i] = ' '.join(words)"
      ],
      "metadata": {
        "id": "vRJ7EiAbg7U1"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_tokenized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3lYAuNni5Qs",
        "outputId": "ef8b971d-c6de-4507-9996-78050ec0bc1e"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mr. green kill colonel mustard studi candlestick .',\n",
              " \"mr. green nice fellow , professor plum green plant studi , miss scarlett water professor plum 's green plant away offic last week .\"]"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatizer with stopwords**"
      ],
      "metadata": {
        "id": "KoRIKzdyjyqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"Mr. Green killed Colonel Mustard in the study with the candlestick. Mr. Green is not a very nice fellow,\n",
        "    Professor Plum has a green plant in his study,\n",
        "    Miss Scarlett watered Professor Plum's green plant while he was away from his office last week.\"\"\""
      ],
      "metadata": {
        "id": "6i3G8Ey-jngx"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_tokenized = nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "x2gFGnmfkRVk"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "qXC3WYEiksxl"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences_tokenized)):\n",
        "  words = nltk.word_tokenize(sentences_tokenized[i])\n",
        "  words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  sentences_tokenized[i] = ' '.join(words)"
      ],
      "metadata": {
        "id": "WHaGMDLXjrlm"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_tokenized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcgCe2TFkaTI",
        "outputId": "97a37f61-8357-4c19-b7c7-b8a177b927bc"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mr. Green killed Colonel Mustard study candlestick .',\n",
              " \"Mr. Green nice fellow , Professor Plum green plant study , Miss Scarlett watered Professor Plum 's green plant away office last week .\"]"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bag Of Words**"
      ],
      "metadata": {
        "id": "w7FAVoaHm0fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"Mr. Green killed Colonel Mustard in the study with the candlestick. Mr. Green is not a very nice fellow,\n",
        "    Professor Plum has a green plant in his study,\n",
        "    Miss Scarlett watered Professor Plum's green plant while he was away from his office last week.\"\"\""
      ],
      "metadata": {
        "id": "ZNioRMbTnWyY"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_tokenized = nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "wqTZiR4mnYlX"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()\n",
        "wordnet = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "MkvkoCThm3jX"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = []"
      ],
      "metadata": {
        "id": "8OyJXD8moDJ6"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences_tokenized)):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', sentences_tokenized[i]) # removing all symbols\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)"
      ],
      "metadata": {
        "id": "miQFqBeRng6A"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DJCL9l4oPuk",
        "outputId": "60c48ca9-58e8-4885-dbee-a2b35b7c362d"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mr green kill colonel mustard studi candlestick',\n",
              " 'mr green kill colonel mustard studi candlestick',\n",
              " 'mr green nice fellow professor plum green plant studi miss scarlett water professor plum green plant away offic last week',\n",
              " 'mr green killed colonel mustard study candlestick',\n",
              " 'mr green nice fellow professor plum green plant study miss scarlett watered professor plum green plant away office last week']"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the Bag of Words model\n",
        "#from sklearn.feature_extraction.text import CountVectorizer\n",
        "#cv = CountVectorizer()\n",
        "#X = cv.fit_transform(corpus).toarray()\n",
        "\n",
        "# Creating the TF-IDF model\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "cv = TfidfVectorizer()\n",
        "X = cv.fit_transform(corpus).toarray()"
      ],
      "metadata": {
        "id": "_C3yofh2n7EI"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inhiNN2bpGHM",
        "outputId": "dffb6a03-9a93-4280-cfc8-fd70f9435441"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.39333024, 0.39333024, 0.        , 0.27985771,\n",
              "        0.47384029, 0.        , 0.        , 0.        , 0.27985771,\n",
              "        0.39333024, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.39333024, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.39333024, 0.39333024, 0.        , 0.27985771,\n",
              "        0.47384029, 0.        , 0.        , 0.        , 0.27985771,\n",
              "        0.39333024, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.39333024, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.19518032, 0.        , 0.        , 0.19518032, 0.34582993,\n",
              "        0.        , 0.        , 0.19518032, 0.19518032, 0.11527664,\n",
              "        0.        , 0.19518032, 0.24192098, 0.        , 0.39036065,\n",
              "        0.39036065, 0.39036065, 0.19518032, 0.1620173 , 0.        ,\n",
              "        0.24192098, 0.        , 0.19518032],\n",
              "       [0.        , 0.36053075, 0.36053075, 0.        , 0.2565206 ,\n",
              "        0.        , 0.53833728, 0.        , 0.        , 0.2565206 ,\n",
              "        0.36053075, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.43432713,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.19403447, 0.        , 0.        , 0.19403447, 0.34379964,\n",
              "        0.        , 0.        , 0.19403447, 0.19403447, 0.11459988,\n",
              "        0.        , 0.19403447, 0.        , 0.24050072, 0.38806893,\n",
              "        0.38806893, 0.38806893, 0.19403447, 0.        , 0.19403447,\n",
              "        0.        , 0.24050072, 0.19403447]])"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    }
  ]
}